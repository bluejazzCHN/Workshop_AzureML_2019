{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML service Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/retkowsky/images/blob/master/AzureMLservicebanniere.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Azure Machine Learning pipeline is an independently executable workflow of a complete machine learning task. Subtasks are encapsulated as a series of steps within the pipeline. An Azure Machine Learning pipeline can be as simple as one that calls a Python script, so may do just about anything. Pipelines should focus on machine learning tasks such as:\n",
    "\n",
    "Data preparation including importing, validating and cleaning, munging and transformation, normalization, and staging\n",
    "Training configuration including parameterizing arguments, filepaths, and logging / reporting configurations\n",
    "Training and validating efficiently and repeatably, which might include specifying specific data subsets, different hardware compute resources, distributed processing, and progress monitoring\n",
    "Deployment, including versioning, scaling, provisioning, and access control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:59:40.925805\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Azure ML service :  1.0.74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"Version Azure ML service : \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline-specific SDK imports\n",
    "\n",
    "Here, we import key pipeline modules, whose use will be illustrated in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline SDK-specific imports completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workshopml\n",
      "workshopmlRG\n",
      "northeurope\n",
      "70b8f39e-8863-49f7-b6ba-34a80799550c\n",
      "Blobstore's name: workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "def_blob_store = ws.get_default_datastore() \n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datastore concepts\n",
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is a place where data can be stored that is then made accessible to a compute either by means of mounting or copying the data to the compute target. \n",
    "\n",
    "A Datastore can either be backed by an Azure File Storage (default) or by an Azure Blob Storage.\n",
    "\n",
    "In this next step, we will upload the training and test set into the workspace's default storage (File storage), and another piece of data to Azure Blob Storage. When to use [Azure Blobs](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction), [Azure Files](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction), or [Azure Disks](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/managed-disks-overview) is [detailed here](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks).\n",
    "\n",
    "**Please take good note of the concept of the datastore.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./20news.pkl\n",
      "Uploaded ./20news.pkl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Upload call completed\n"
     ]
    }
   ],
   "source": [
    "# get_default_datastore() gets the default Azure Blob Store associated with your workspace.\n",
    "# Here we are reusing the def_blob_store object we obtained earlier\n",
    "def_blob_store.upload_files([\"./20news.pkl\"], target_path=\"20newsgroups\", overwrite=True)\n",
    "print(\"Upload call completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Targets\n",
    "A compute target specifies where to execute your program such as a remote Docker on a VM, or a cluster. A compute target needs to be addressable and accessible by you.\n",
    "\n",
    "**You need at least one compute target to send your payload to. We are planning to use Azure Machine Learning Compute exclusively for this tutorial for all steps. However in some cases you may require multiple compute targets as some steps may run in one compute target like Azure Machine Learning Compute, and some other steps in the same pipeline could run in a different compute target.**\n",
    "\n",
    "*The example belows show creating/retrieving/attaching to an Azure Machine Learning Compute instance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Compute Targets on the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardds13V2\n",
      "cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve or create a Azure Machine Learning compute\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Azure Machine Learning Compute in the current workspace, if it doesn't already exist. We will then run the training script on this compute target.\n",
    "\n",
    "If we could not find the compute with the given name in the previous cell, then we will create a new compute here. We will create an Azure Machine Learning Compute containing **STANDARD_D2_V2 CPU VMs**. This process is broken down into the following steps:\n",
    "\n",
    "1. Create the configuration\n",
    "2. Create the Azure Machine Learning compute\n",
    "\n",
    "**This process will take about 3 minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found existing compute target.\n",
      "Azure Machine Learning Compute attached\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-11-18T10:54:22.315000+00:00', 'errors': None, 'creationTime': '2019-11-18T10:53:46.968354+00:00', 'modifiedTime': '2019-11-18T10:54:32.587183+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
    "# example: un-comment the following line.\n",
    "print(aml_compute.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Step in a Pipeline\n",
    "A Step is a unit of execution. Step typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. Azure Machine Learning Pipelines provides the following built-in Steps:\n",
    "\n",
    "- [**PythonScriptStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py): Adds a step to run a Python script in a Pipeline.\n",
    "- [**AdlaStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.adla_step.adlastep?view=azure-ml-py): Adds a step to run U-SQL script using Azure Data Lake Analytics.\n",
    "- [**DataTransferStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.data_transfer_step.datatransferstep?view=azure-ml-py): Transfers data between Azure Blob and Data Lake accounts.\n",
    "- [**DatabricksStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py): Adds a DataBricks notebook as a step in a Pipeline.\n",
    "- [**HyperDriveStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.hyper_drive_step.hyperdrivestep?view=azure-ml-py): Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.\n",
    "- [**AzureBatchStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.azurebatch_step.azurebatchstep?view=azure-ml-py): Creates a step for submitting jobs to Azure Batch\n",
    "- [**EstimatorStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.estimator_step.estimatorstep?view=azure-ml-py): Adds a step to run Estimator in a Pipeline.\n",
    "- [**MpiStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.mpi_step.mpistep?view=azure-ml-py): Adds a step to run a MPI job in a Pipeline.\n",
    "- [**AutoMLStep**](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlstep?view=azure-ml-py): Creates a AutoML step in a Pipeline.\n",
    "\n",
    "The following code will create a PythonScriptStep to be executed in the Azure Machine Learning Compute we created above using train.py, one of the files already made available in the `source_directory`.\n",
    "\n",
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify requirements for the PythonScriptStep, such as conda dependencies and docker image.\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source directory for the step is /mnt/azmnt/code/Users/Workshop/scripts.\n",
      "Step1 created\n"
     ]
    }
   ],
   "source": [
    "source_directory = './scripts'\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "\n",
    "# Syntax\n",
    "# PythonScriptStep(\n",
    "#     script_name, \n",
    "#     name=None, \n",
    "#     arguments=None, \n",
    "#     compute_target=None, \n",
    "#     runconfig=None, \n",
    "#     inputs=None, \n",
    "#     outputs=None, \n",
    "#     params=None, \n",
    "#     source_directory=None, \n",
    "#     allow_reuse=True, \n",
    "#     version=None, \n",
    "#     hash_paths=None)\n",
    "# This returns a Step\n",
    "step1 = PythonScriptStep(name=\"1_train_step\",\n",
    "                         script_name=\"train.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory,\n",
    "                         allow_reuse=True)\n",
    "print(\"Step1 created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the above call to PythonScriptStep(), the flag *allow_reuse* determines whether the step should reuse previous results when run with the same settings/inputs. This flag's default value is *True*; the default is set to *True* because, when inputs and parameters have not changed, we typically do not want to re-run a given pipeline step. \n",
    "\n",
    "If *allow_reuse* is set to *False*, a new run will always be generated for this step during pipeline execution. The *allow_reuse* flag can come in handy in situations where you do *not* want to re-run a pipeline step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a few steps in parallel\n",
    "Here we are looking at a simple scenario where we are running a few steps (all involving PythonScriptStep)  in parallel. Running nodes in **parallel** is the default behavior for steps in a pipeline.\n",
    "\n",
    "We already have one step defined earlier. Let's define few more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source directory for the step is /mnt/azmnt/code/Users/Workshop/scripts.\n",
      "Source directory for the step is /mnt/azmnt/code/Users/Workshop/scripts.\n",
      "Step lists created\n"
     ]
    }
   ],
   "source": [
    "# For this step, we use a different source_directory\n",
    "source_directory = './scripts'\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "\n",
    "# All steps use the same Azure Machine Learning compute target as well\n",
    "step2 = PythonScriptStep(name=\"2_compare_step\",\n",
    "                         script_name=\"compare.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory)\n",
    "\n",
    "# Use a RunConfiguration to specify some additional requirements for this step.\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "\n",
    "# For this step, we use yet another source_directory\n",
    "source_directory = './scripts'\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "\n",
    "step3 = PythonScriptStep(name=\"3_extract_step\",\n",
    "                         script_name=\"extract.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory,\n",
    "                         runconfig=run_config)\n",
    "\n",
    "# list of steps to run\n",
    "steps = [step1, step2, step3]\n",
    "print(\"Step lists created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py). By deafult, all these steps will run in **parallel** once we submit the pipeline for run.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py#submit-config--tags-none----kwargs-). When submit is called, a [PipelineRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n"
     ]
    }
   ],
   "source": [
    "# Syntax\n",
    "# Pipeline(workspace, \n",
    "#          steps, \n",
    "#          description=None, \n",
    "#          default_datastore_name=None, \n",
    "#          default_source_directory=None, \n",
    "#          resolve_closure=True, \n",
    "#          _workflow_provider=None, \n",
    "#          _service_endpoint=None)\n",
    "\n",
    "pipeline1 = Pipeline(workspace=ws, steps=steps)\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the pipeline\n",
    "You have the option to [validate](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#validate--) the pipeline prior to submitting for run. The platform runs validation steps such as checking for circular dependencies and parameter checks etc. even if you do not explicitly call validate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1_train_step is ready to be created [e717e67d]\n",
      "Step 2_compare_step is ready to be created [42331156]\n",
      "Step 3_extract_step is ready to be created [a1d23260]\n",
      "Pipeline validation complete\n"
     ]
    }
   ],
   "source": [
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline\n",
    "[Submitting](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#submit) the pipeline involves creating an [Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment?view=azure-ml-py) object and providing the built pipeline for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step 1_train_step [e717e67d][bbc74b99-c9aa-42a1-b3b7-155eb8d86a64], (This step will run and generate new outputs)\n",
      "Created step 2_compare_step [42331156][a3d9607d-0c26-4e0f-ba74-e77a0d47f0c1], (This step will run and generate new outputs)\n",
      "Created step 3_extract_step [a1d23260][e22fba6a-7100-4344-9e70-c66b1512e341], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 1c230923-9311-4052-9214-1f977a0609e1\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/Pipeline1/runs/1c230923-9311-4052-9214-1f977a0609e1?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/workshopmlRG/workspaces/workshopml\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "# Submit syntax\n",
    "# submit(experiment_name, \n",
    "#        pipeline_parameters=None, \n",
    "#        continue_on_step_failure=False, \n",
    "#        regenerate_outputs=False)\n",
    "\n",
    "pipeline_run1 = Experiment(ws, 'Pipeline1').submit(pipeline1, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If regenerate_outputs is set to True, a new submit will always force generation of all step outputs, and disallow data reuse for any step of this run. Once this run is complete, however, subsequent runs may reuse the results of this run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the pipeline run\n",
    "\n",
    "#### Use RunDetails Widget\n",
    "We are going to use the RunDetails widget to examine the run of the pipeline. You can click each row below to get more details on the step runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf5445294dc4e3fa371be796849c092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/Pipeline1/runs/1c230923-9311-4052-9214-1f977a0609e1?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/workshopmlRG/workspaces/workshopml\", \"run_id\": \"1c230923-9311-4052-9214-1f977a0609e1\", \"run_properties\": {\"run_id\": \"1c230923-9311-4052-9214-1f977a0609e1\", \"created_utc\": \"2019-11-18T11:00:27.372922Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": null, \"runType\": \"HTTP\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2019-11-18T11:08:31.381732Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://workshopml4801169773.blob.core.windows.net/azureml/ExperimentRun/dcid.1c230923-9311-4052-9214-1f977a0609e1/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=a3Ak1BxZHTwfjigk557QjlAEE7N3vCQ%2BlyDz5e1%2FuPM%3D&st=2019-11-18T10%3A58%3A58Z&se=2019-11-18T19%3A08%3A58Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://workshopml4801169773.blob.core.windows.net/azureml/ExperimentRun/dcid.1c230923-9311-4052-9214-1f977a0609e1/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=NEi%2Borth%2FhEU7Q779juxAWu0P4IFEEbyYb8kI%2FuZHoE%3D&st=2019-11-18T10%3A58%3A58Z&se=2019-11-18T19%3A08%3A58Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://workshopml4801169773.blob.core.windows.net/azureml/ExperimentRun/dcid.1c230923-9311-4052-9214-1f977a0609e1/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=KljE57UQvBxfQlN64XpRBJOiGCgjpa3kAU7PyZRC3Y4%3D&st=2019-11-18T10%3A58%3A58Z&se=2019-11-18T19%3A08%3A58Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:08:04\"}, \"child_runs\": [{\"run_id\": \"52e5497f-c16a-4ade-b17a-dfba8f690997\", \"name\": \"1_train_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:05:21.337527Z\", \"created_time\": \"2019-11-18T11:00:36.610347Z\", \"end_time\": \"2019-11-18T11:06:26.805746Z\", \"duration\": \"0:05:50\", \"run_number\": 2, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:00:36.610347Z\", \"is_reused\": \"\"}, {\"run_id\": \"fdab6965-8718-4597-8f51-753cf549c5f5\", \"name\": \"2_compare_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:06:26.856502Z\", \"created_time\": \"2019-11-18T11:00:36.615997Z\", \"end_time\": \"2019-11-18T11:07:20.214016Z\", \"duration\": \"0:06:43\", \"run_number\": 3, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:00:36.615997Z\", \"is_reused\": \"\"}, {\"run_id\": \"c124f61c-5fe2-4f25-a7e0-1442a7d96448\", \"name\": \"3_extract_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:07:19.848974Z\", \"created_time\": \"2019-11-18T11:00:36.618412Z\", \"end_time\": \"2019-11-18T11:08:20.290921Z\", \"duration\": \"0:07:43\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:00:36.618412Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2019-11-18 11:00:36Z] Submitting run id fdab6965-8718-4597-8f51-753cf549c5f5 in experiment Pipeline1\\n[2019-11-18 11:00:36Z] Submitting run id c124f61c-5fe2-4f25-a7e0-1442a7d96448 in experiment Pipeline1\\n[2019-11-18 11:00:36Z] Submitting run id 52e5497f-c16a-4ade-b17a-dfba8f690997 in experiment Pipeline1\\n[2019-11-18 11:06:29Z] Completing processing run id 52e5497f-c16a-4ade-b17a-dfba8f690997.\\n[2019-11-18 11:07:24Z] Completing processing run id fdab6965-8718-4597-8f51-753cf549c5f5.\\n[2019-11-18 11:08:27Z] Completing processing run id c124f61c-5fe2-4f25-a7e0-1442a7d96448.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {}, \"module_nodes\": {\"e717e67d\": {\"node_id\": \"e717e67d\", \"name\": \"1_train_step\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"52e5497f-c16a-4ade-b17a-dfba8f690997\"}, \"42331156\": {\"node_id\": \"42331156\", \"name\": \"2_compare_step\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"fdab6965-8718-4597-8f51-753cf549c5f5\"}, \"a1d23260\": {\"node_id\": \"a1d23260\", \"name\": \"3_extract_step\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"c124f61c-5fe2-4f25-a7e0-1442a7d96448\"}}, \"edges\": [], \"child_runs\": [{\"run_id\": \"52e5497f-c16a-4ade-b17a-dfba8f690997\", \"name\": \"1_train_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:05:21.337527Z\", \"created_time\": \"2019-11-18T11:00:36.610347Z\", \"end_time\": \"2019-11-18T11:06:26.805746Z\", \"duration\": \"0:05:50\", \"run_number\": 2, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:00:36.610347Z\", \"is_reused\": \"\"}, {\"run_id\": \"fdab6965-8718-4597-8f51-753cf549c5f5\", \"name\": \"2_compare_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:06:26.856502Z\", \"created_time\": \"2019-11-18T11:00:36.615997Z\", \"end_time\": \"2019-11-18T11:07:20.214016Z\", \"duration\": \"0:06:43\", \"run_number\": 3, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:00:36.615997Z\", \"is_reused\": \"\"}, {\"run_id\": \"c124f61c-5fe2-4f25-a7e0-1442a7d96448\", \"name\": \"3_extract_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:07:19.848974Z\", \"created_time\": \"2019-11-18T11:00:36.618412Z\", \"end_time\": \"2019-11-18T11:08:20.290921Z\", \"duration\": \"0:07:43\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:00:36.618412Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.74\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline SDK objects\n",
    "You can cycle through the node_run objects and examine job logs, stdout, and stderr of each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script: 3_extract_step status: Finished\n",
      "Script: 2_compare_step status: Finished\n",
      "Script: 1_train_step status: Finished\n"
     ]
    }
   ],
   "source": [
    "step_runs = pipeline_run1.get_children()\n",
    "for step_run in step_runs:\n",
    "    status = step_run.get_status()\n",
    "    print('Script:', step_run.name, 'status:', status)\n",
    "    \n",
    "    # Change this if you want to see details even if the Step has succeeded.\n",
    "    if status == \"Failed\":\n",
    "        joblog = step_run.get_job_log()\n",
    "        print('job log:', joblog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a few steps in sequence\n",
    "Now let's see how we run a few steps in sequence. We already have three steps defined earlier. Let's *reuse* those steps for this part.\n",
    "\n",
    "We will reuse step1, step2, step3, but build the pipeline in such a way that we chain step3 after step2 and step2 after step1. Note that there is no explicit data dependency between these steps, but still steps can be made dependent by using the [run_after](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.builder.pipelinestep?view=azure-ml-py#run-after-step-) construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n",
      "Step 3_extract_step is ready to be created [56736f57]\n",
      "Step 2_compare_step is ready to be created [8db35fb0]\n",
      "Simple validation complete\n"
     ]
    }
   ],
   "source": [
    "step2.run_after(step1)\n",
    "step3.run_after(step2)\n",
    "\n",
    "# Try a loop\n",
    "#step2.run_after(step3)\n",
    "\n",
    "# Now, construct the pipeline using the steps.\n",
    "\n",
    "# We can specify the \"final step\" in the chain, \n",
    "# Pipeline will take care of \"transitive closure\" and \n",
    "# figure out the implicit or explicit dependencies\n",
    "# https://www.geeksforgeeks.org/transitive-closure-of-a-graph/\n",
    "pipeline2 = Pipeline(workspace=ws, steps=[step3])\n",
    "print (\"Pipeline is built\")\n",
    "\n",
    "pipeline2.validate()\n",
    "print(\"Simple validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step 3_extract_step [56736f57][a1f5cfde-f722-4648-8ca7-aefd4d296b47], (This step will run and generate new outputs)\n",
      "Created step 2_compare_step [8db35fb0][b69447f7-8f4b-4390-8d2a-73d24725cab4], (This step will run and generate new outputs)\n",
      "Created step 1_train_step [a461c36f][bbc74b99-c9aa-42a1-b3b7-155eb8d86a64], (This step is eligible to reuse a previous run's output)\n",
      "Submitted PipelineRun 0d2db162-0779-492b-8016-17dac3cff8d4\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/Pipeline2/runs/0d2db162-0779-492b-8016-17dac3cff8d4?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/workshopmlRG/workspaces/workshopml\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run2 = Experiment(ws, 'Pipeline2').submit(pipeline2)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335996c32c5a489e8a428fdb44b2886a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/Pipeline2/runs/0d2db162-0779-492b-8016-17dac3cff8d4?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/workshopmlRG/workspaces/workshopml\", \"run_id\": \"0d2db162-0779-492b-8016-17dac3cff8d4\", \"run_properties\": {\"run_id\": \"0d2db162-0779-492b-8016-17dac3cff8d4\", \"created_utc\": \"2019-11-18T11:10:32.989298Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": null, \"runType\": \"HTTP\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2019-11-18T11:14:12.787824Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://workshopml4801169773.blob.core.windows.net/azureml/ExperimentRun/dcid.0d2db162-0779-492b-8016-17dac3cff8d4/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=Bpordnw1Eo5FGxNuOCRzkz19htSbpbZdPvvNxQkyP%2B4%3D&st=2019-11-18T11%3A04%3A26Z&se=2019-11-18T19%3A14%3A26Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://workshopml4801169773.blob.core.windows.net/azureml/ExperimentRun/dcid.0d2db162-0779-492b-8016-17dac3cff8d4/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=9ocGoOJX94DmMGBD6doC53%2FdN3DEM0s42X6ucKdIa0c%3D&st=2019-11-18T11%3A04%3A26Z&se=2019-11-18T19%3A14%3A26Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://workshopml4801169773.blob.core.windows.net/azureml/ExperimentRun/dcid.0d2db162-0779-492b-8016-17dac3cff8d4/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=n%2FTpfASdDXvFD5QleeOrVauor23AN47GiII26TD3ScI%3D&st=2019-11-18T11%3A04%3A26Z&se=2019-11-18T19%3A14%3A26Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:03:39\"}, \"child_runs\": [{\"run_id\": \"010151a3-1c9b-48dd-97c7-8305ef02dd96\", \"name\": \"3_extract_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:13:06.61523Z\", \"created_time\": \"2019-11-18T11:12:47.966358Z\", \"end_time\": \"2019-11-18T11:14:06.777083Z\", \"duration\": \"0:01:18\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:12:47.966358Z\", \"is_reused\": \"\"}, {\"run_id\": \"f418fefc-99e0-4edd-aea6-8809d810eebb\", \"name\": \"2_compare_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:10:53.738389Z\", \"created_time\": \"2019-11-18T11:10:38.232757Z\", \"end_time\": \"2019-11-18T11:12:42.985445Z\", \"duration\": \"0:02:04\", \"run_number\": 3, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:10:38.232757Z\", \"is_reused\": \"\"}, {\"run_id\": \"41b973d4-1e6f-402b-8441-986c865e5041\", \"name\": \"1_train_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:10:37.530874Z\", \"created_time\": \"2019-11-18T11:10:37.530874Z\", \"end_time\": \"2019-11-18T11:10:37.58186Z\", \"duration\": \"0:00:00\", \"run_number\": 2, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:10:37.530874Z\", \"is_reused\": \"Yes\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2019-11-18 11:10:37Z] Completing processing run id 41b973d4-1e6f-402b-8441-986c865e5041.\\n[2019-11-18 11:10:38Z] Submitting run id f418fefc-99e0-4edd-aea6-8809d810eebb in experiment Pipeline2\\n[2019-11-18 11:12:46Z] Completing processing run id f418fefc-99e0-4edd-aea6-8809d810eebb.\\n[2019-11-18 11:12:46Z] Submitting run id 010151a3-1c9b-48dd-97c7-8305ef02dd96 in experiment Pipeline2\\n[2019-11-18 11:14:12Z] Completing processing run id 010151a3-1c9b-48dd-97c7-8305ef02dd96.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {}, \"module_nodes\": {\"56736f57\": {\"node_id\": \"56736f57\", \"name\": \"3_extract_step\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"010151a3-1c9b-48dd-97c7-8305ef02dd96\"}, \"8db35fb0\": {\"node_id\": \"8db35fb0\", \"name\": \"2_compare_step\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"f418fefc-99e0-4edd-aea6-8809d810eebb\"}, \"a461c36f\": {\"node_id\": \"a461c36f\", \"name\": \"1_train_step\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"41b973d4-1e6f-402b-8441-986c865e5041\"}}, \"edges\": [{\"source_node_id\": \"8db35fb0\", \"source_node_name\": \"2_compare_step\", \"source_name\": \"_run_after_output\", \"target_name\": \"_run_after_input_0\", \"dst_node_id\": \"56736f57\", \"dst_node_name\": \"3_extract_step\"}, {\"source_node_id\": \"a461c36f\", \"source_node_name\": \"1_train_step\", \"source_name\": \"_run_after_output\", \"target_name\": \"_run_after_input_0\", \"dst_node_id\": \"8db35fb0\", \"dst_node_name\": \"2_compare_step\"}], \"child_runs\": [{\"run_id\": \"010151a3-1c9b-48dd-97c7-8305ef02dd96\", \"name\": \"3_extract_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:13:06.61523Z\", \"created_time\": \"2019-11-18T11:12:47.966358Z\", \"end_time\": \"2019-11-18T11:14:06.777083Z\", \"duration\": \"0:01:18\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:12:47.966358Z\", \"is_reused\": \"\"}, {\"run_id\": \"f418fefc-99e0-4edd-aea6-8809d810eebb\", \"name\": \"2_compare_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:10:53.738389Z\", \"created_time\": \"2019-11-18T11:10:38.232757Z\", \"end_time\": \"2019-11-18T11:12:42.985445Z\", \"duration\": \"0:02:04\", \"run_number\": 3, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:10:38.232757Z\", \"is_reused\": \"\"}, {\"run_id\": \"41b973d4-1e6f-402b-8441-986c865e5041\", \"name\": \"1_train_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-18T11:10:37.530874Z\", \"created_time\": \"2019-11-18T11:10:37.530874Z\", \"end_time\": \"2019-11-18T11:10:37.58186Z\", \"duration\": \"0:00:00\", \"run_number\": 2, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-18T11:10:37.530874Z\", \"is_reused\": \"Yes\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.74\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script: 3_extract_step status: Finished\n",
      "Script: 2_compare_step status: Finished\n",
      "Script: 1_train_step status: Finished\n"
     ]
    }
   ],
   "source": [
    "step_runs = pipeline_run2.get_children()\n",
    "for step_run in step_runs:\n",
    "    status = step_run.get_status()\n",
    "    print('Script:', step_run.name, 'status:', status)\n",
    "    \n",
    "    # Change this if you want to see details even if the Step has succeeded.\n",
    "    if status == \"Failed\":\n",
    "        joblog = step_run.get_job_log()\n",
    "        print('job log:', joblog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/retkowsky/images/blob/master/Powered-by-MS-Azure-logo-v2.png?raw=true\" height=\"300\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Getting Started with Azure Machine Learning Pipelines",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "order_index": 1,
  "tags": [
   "None"
  ],
  "task": "Getting Started notebook for ANML Pipelines"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
